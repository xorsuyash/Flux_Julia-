{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef9fc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suyash\n"
     ]
    }
   ],
   "source": [
    "println(\"suyash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c4c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux , Images , MLDatasets , Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c9883a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux: crossentropy , onecold , onehotbatch , train!,params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2105ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra , Random , Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0366c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskLocalRNG()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db06e4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mMNIST.traindata() is deprecated, use `MNIST(split=:train)[:]` instead.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLDatasets ~/.julia/packages/MLDatasets/vws5b/src/datasets/vision/mnist.jl:187\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(features = [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; … ;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], targets = [5, 0, 4, 1, 9, 2, 1, 3, 1, 4  …  9, 2, 9, 5, 1, 8, 3, 5, 6, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_raw ,y_train_raw =MLDatasets.MNIST.traindata(Float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba2ddb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mMNIST.testdata() is deprecated, use `MNIST(split=:test)[:]` instead.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLDatasets ~/.julia/packages/MLDatasets/vws5b/src/datasets/vision/mnist.jl:195\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(features = [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; … ;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], targets = [7, 2, 1, 0, 4, 1, 4, 9, 5, 9  …  7, 8, 9, 0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_raw ,y_test_raw =MLDatasets.MNIST.testdata(Float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c313e79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188160000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizeof(X_train_raw )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd9bb863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAgVJREFUaAW9wb2LFgQAB+Dn4idIQoHeIqFOLZbQcIMfYOEf4GZ24SChJEabIOggKIqLRTR1+BHiJCHoGhKBg6vQ0NAtKg0J6tSi3Dnc8PJyvnfve+rveaIsyqIsyqIsyqIsyqIsyqIsyqIsyqIsyuIduYsp7DMsyqIsyuId+BG7cd1yURZlURZv2UUcwwvctVyURVmUxVu2E+twDzctF2VRFmUxob04jVk8NWwWn2IeJ7xelEVZlMWE5vAxtuOeYaexCUfxwOtFWZRFWUzofyxivWGfYSsWsN5oURZlURYTOIcd+BsPDGzASbyP+/jNaFEWZVEWY9qCo3iJ7/DEwA84gH+xx8qiLMqiLMawA7cwjZ/xp4ETOGzJeauLsiiLslhBcAhX8B4WsAuncAkbcQBTuI5frC7KoizKYgVf4TIWsYB/MIMZ7MdH2Iwn+MZ4oizKoixGOIhreIHn+BrPcAmfYwZTWMQ0HuELzFtZlEVZlMUI3+IhzuOqge8xh50GpvAH5q0uyqIsymKE27iFR4ZN4xNLZvGXJY+NJ8qiLMpihJ8s9yG+xAeYx02Ti7Ioi7KYwHEcw3/YZ22iLMqiLMa0DUewiDk8tjZRFmVRFmP6HdtwA2esXZRFWZTFmH7FWdzxZqIsyqIsxnQBF7y5KIuyKIuyKIuyKIuyKIuyKIuyKIuyKHsF5pFJRh6u9JIAAAAASUVORK5CYII=",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAgVJREFUaAW9wb2LFgQAB+Dn4idIQoHeIqFOLZbQcIMfYOEf4GZ24SChJEabIOggKIqLRTR1+BHiJCHoGhKBg6vQ0NAtKg0J6tSi3Dnc8PJyvnfve+rveaIsyqIsyqIsyqIsyqIsyqIsyqIsyqIsyuIduYsp7DMsyqIsyuId+BG7cd1yURZlURZv2UUcwwvctVyURVmUxVu2E+twDzctF2VRFmUxob04jVk8NWwWn2IeJ7xelEVZlMWE5vAxtuOeYaexCUfxwOtFWZRFWUzofyxivWGfYSsWsN5oURZlURYTOIcd+BsPDGzASbyP+/jNaFEWZVEWY9qCo3iJ7/DEwA84gH+xx8qiLMqiLMawA7cwjZ/xp4ETOGzJeauLsiiLslhBcAhX8B4WsAuncAkbcQBTuI5frC7KoizKYgVf4TIWsYB/MIMZ7MdH2Iwn+MZ4oizKoixGOIhreIHn+BrPcAmfYwZTWMQ0HuELzFtZlEVZlMUI3+IhzuOqge8xh50GpvAH5q0uyqIsymKE27iFR4ZN4xNLZvGXJY+NJ8qiLMpihJ8s9yG+xAeYx02Ti7Ioi7KYwHEcw3/YZ22iLMqiLMa0DUewiDk8tjZRFmVRFmP6HdtwA2esXZRFWZTFmH7FWdzxZqIsyqIsxnQBF7y5KIuyKIuyKIuyKIuyKIuyKIuyKIuyKHsF5pFJRh6u9JIAAAAASUVORK5C\">"
      ],
      "text/plain": [
       "28×28 reinterpret(reshape, Gray{Float32}, adjoint(::Matrix{Float32})) with eltype Gray{Float32}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱  \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=10 \n",
    "\n",
    "img= X_train_raw[:,:,index]\n",
    "colorview(Gray , img')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcb2f604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_raw[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88fd2ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784×60000 Matrix{Float32}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱            ⋮                   \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   X_train=Flux.flatten(X_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25b6f14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784×10000 Matrix{Float32}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱            ⋮                   \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=Flux.flatten(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a220d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×60000 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n",
       " ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  …  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  1     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  …  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  1  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  1\n",
       " ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=onehotbatch(y_train_raw , 0:9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4cbf24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10000 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n",
       " ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  …  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  …  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅     ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1\n",
       " 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  1     ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test =onehotbatch(y_test_raw , 0:9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bcabda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784 => 32, relu),               \u001b[90m# 25_120 parameters\u001b[39m\n",
       "  Dense(32 => 10),                      \u001b[90m# 330 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m25_450 parameters, 99.664 KiB."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Chain(\n",
    "    Dense(28*28 ,32 , relu),\n",
    "      Dense(32,10),\n",
    "       softmax )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfed54c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(x,y)= crossentropy(model(x),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "431a4094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[-0.08387847 -0.05129739 … 0.01803515 0.019105801; -0.077316396 0.01100761 … -0.012784295 0.0450724; … ; 0.037071276 0.040087618 … 0.08153726 0.054491512; 0.051630527 -0.011924278 … 0.08569224 0.04504712], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.15744792 -0.07656168 … -0.04842384 -0.13414541; -0.3762223 0.0552824 … -0.12552322 -0.2822265; … ; 0.1961483 -0.13112722 … -0.2807336 0.3678737; 0.24970926 -0.3143418 … 0.14662431 -0.054380406], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ps=params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66f8a08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam(0.01, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}())"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.01\n",
    "opt = ADAM(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a083725f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_history=[]\n",
    "epochs= 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "516412ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 : Training Loss = 0.7877831\n",
      "Epoch = 2 : Training Loss = 0.67078894\n",
      "Epoch = 3 : Training Loss = 0.5911647\n",
      "Epoch = 4 : Training Loss = 0.5364811\n",
      "Epoch = 5 : Training Loss = 0.49772683\n",
      "Epoch = 6 : Training Loss = 0.4683123\n",
      "Epoch = 7 : Training Loss = 0.44480518\n",
      "Epoch = 8 : Training Loss = 0.42594695\n",
      "Epoch = 9 : Training Loss = 0.41077128\n",
      "Epoch = 10 : Training Loss = 0.3963644\n",
      "Epoch = 11 : Training Loss = 0.38463184\n",
      "Epoch = 12 : Training Loss = 0.3747828\n",
      "Epoch = 13 : Training Loss = 0.36559424\n",
      "Epoch = 14 : Training Loss = 0.35712177\n",
      "Epoch = 15 : Training Loss = 0.3490166\n",
      "Epoch = 16 : Training Loss = 0.34206536\n",
      "Epoch = 17 : Training Loss = 0.33518124\n",
      "Epoch = 18 : Training Loss = 0.32819563\n",
      "Epoch = 19 : Training Loss = 0.32181603\n",
      "Epoch = 20 : Training Loss = 0.3158584\n",
      "Epoch = 21 : Training Loss = 0.31042442\n",
      "Epoch = 22 : Training Loss = 0.30507693\n",
      "Epoch = 23 : Training Loss = 0.29985964\n",
      "Epoch = 24 : Training Loss = 0.29497296\n",
      "Epoch = 25 : Training Loss = 0.29014713\n",
      "Epoch = 26 : Training Loss = 0.28547773\n",
      "Epoch = 27 : Training Loss = 0.28087226\n",
      "Epoch = 28 : Training Loss = 0.2764769\n",
      "Epoch = 29 : Training Loss = 0.2723249\n",
      "Epoch = 30 : Training Loss = 0.2683595\n",
      "Epoch = 31 : Training Loss = 0.26463598\n",
      "Epoch = 32 : Training Loss = 0.26100138\n",
      "Epoch = 33 : Training Loss = 0.25755215\n",
      "Epoch = 34 : Training Loss = 0.25421906\n",
      "Epoch = 35 : Training Loss = 0.25096998\n",
      "Epoch = 36 : Training Loss = 0.24782051\n",
      "Epoch = 37 : Training Loss = 0.24479204\n",
      "Epoch = 38 : Training Loss = 0.24193044\n",
      "Epoch = 39 : Training Loss = 0.23913068\n",
      "Epoch = 40 : Training Loss = 0.23644766\n",
      "Epoch = 41 : Training Loss = 0.23385824\n",
      "Epoch = 42 : Training Loss = 0.231331\n",
      "Epoch = 43 : Training Loss = 0.22882317\n",
      "Epoch = 44 : Training Loss = 0.22635294\n",
      "Epoch = 45 : Training Loss = 0.2239264\n",
      "Epoch = 46 : Training Loss = 0.22152191\n",
      "Epoch = 47 : Training Loss = 0.21915054\n",
      "Epoch = 48 : Training Loss = 0.21682881\n",
      "Epoch = 49 : Training Loss = 0.21457969\n",
      "Epoch = 50 : Training Loss = 0.2123592\n",
      "Epoch = 51 : Training Loss = 0.21019296\n",
      "Epoch = 52 : Training Loss = 0.20807132\n",
      "Epoch = 53 : Training Loss = 0.20600271\n",
      "Epoch = 54 : Training Loss = 0.20396373\n",
      "Epoch = 55 : Training Loss = 0.20197915\n",
      "Epoch = 56 : Training Loss = 0.2000358\n",
      "Epoch = 57 : Training Loss = 0.19814238\n",
      "Epoch = 58 : Training Loss = 0.19628425\n",
      "Epoch = 59 : Training Loss = 0.19446631\n",
      "Epoch = 60 : Training Loss = 0.19267859\n",
      "Epoch = 61 : Training Loss = 0.19092757\n",
      "Epoch = 62 : Training Loss = 0.18920884\n",
      "Epoch = 63 : Training Loss = 0.18752332\n",
      "Epoch = 64 : Training Loss = 0.18586852\n",
      "Epoch = 65 : Training Loss = 0.18424554\n",
      "Epoch = 66 : Training Loss = 0.1826508\n",
      "Epoch = 67 : Training Loss = 0.18108949\n",
      "Epoch = 68 : Training Loss = 0.17955837\n",
      "Epoch = 69 : Training Loss = 0.17805557\n",
      "Epoch = 70 : Training Loss = 0.17657536\n",
      "Epoch = 71 : Training Loss = 0.17512617\n",
      "Epoch = 72 : Training Loss = 0.17370452\n",
      "Epoch = 73 : Training Loss = 0.1723109\n",
      "Epoch = 74 : Training Loss = 0.17093913\n",
      "Epoch = 75 : Training Loss = 0.169593\n",
      "Epoch = 76 : Training Loss = 0.16826995\n",
      "Epoch = 77 : Training Loss = 0.16696891\n",
      "Epoch = 78 : Training Loss = 0.16568509\n",
      "Epoch = 79 : Training Loss = 0.16441849\n",
      "Epoch = 80 : Training Loss = 0.16317002\n",
      "Epoch = 81 : Training Loss = 0.16194238\n",
      "Epoch = 82 : Training Loss = 0.16073431\n",
      "Epoch = 83 : Training Loss = 0.15954362\n",
      "Epoch = 84 : Training Loss = 0.15836929\n",
      "Epoch = 85 : Training Loss = 0.15721393\n",
      "Epoch = 86 : Training Loss = 0.15607496\n",
      "Epoch = 87 : Training Loss = 0.15495443\n",
      "Epoch = 88 : Training Loss = 0.15384832\n",
      "Epoch = 89 : Training Loss = 0.1527571\n",
      "Epoch = 90 : Training Loss = 0.15168007\n",
      "Epoch = 91 : Training Loss = 0.15061635\n",
      "Epoch = 92 : Training Loss = 0.14956398\n",
      "Epoch = 93 : Training Loss = 0.14852463\n",
      "Epoch = 94 : Training Loss = 0.14749597\n",
      "Epoch = 95 : Training Loss = 0.14647622\n",
      "Epoch = 96 : Training Loss = 0.14546612\n",
      "Epoch = 97 : Training Loss = 0.14446692\n",
      "Epoch = 98 : Training Loss = 0.14347982\n",
      "Epoch = 99 : Training Loss = 0.14250533\n",
      "Epoch = 100 : Training Loss = 0.14154388\n",
      "Epoch = 101 : Training Loss = 0.14059307\n",
      "Epoch = 102 : Training Loss = 0.13965504\n",
      "Epoch = 103 : Training Loss = 0.13872986\n",
      "Epoch = 104 : Training Loss = 0.1378152\n",
      "Epoch = 105 : Training Loss = 0.13691367\n",
      "Epoch = 106 : Training Loss = 0.1360227\n",
      "Epoch = 107 : Training Loss = 0.13514382\n",
      "Epoch = 108 : Training Loss = 0.13427392\n",
      "Epoch = 109 : Training Loss = 0.13341378\n",
      "Epoch = 110 : Training Loss = 0.13256185\n",
      "Epoch = 111 : Training Loss = 0.13172016\n",
      "Epoch = 112 : Training Loss = 0.13088845\n",
      "Epoch = 113 : Training Loss = 0.1300671\n",
      "Epoch = 114 : Training Loss = 0.12925437\n",
      "Epoch = 115 : Training Loss = 0.12844765\n",
      "Epoch = 116 : Training Loss = 0.12764817\n",
      "Epoch = 117 : Training Loss = 0.12685727\n",
      "Epoch = 118 : Training Loss = 0.1260754\n",
      "Epoch = 119 : Training Loss = 0.12530147\n",
      "Epoch = 120 : Training Loss = 0.124532044\n",
      "Epoch = 121 : Training Loss = 0.123768896\n",
      "Epoch = 122 : Training Loss = 0.12301315\n",
      "Epoch = 123 : Training Loss = 0.12226579\n",
      "Epoch = 124 : Training Loss = 0.12152463\n",
      "Epoch = 125 : Training Loss = 0.12078885\n",
      "Epoch = 126 : Training Loss = 0.12006199\n",
      "Epoch = 127 : Training Loss = 0.11933738\n",
      "Epoch = 128 : Training Loss = 0.11861875\n",
      "Epoch = 129 : Training Loss = 0.11790699\n",
      "Epoch = 130 : Training Loss = 0.11720226\n",
      "Epoch = 131 : Training Loss = 0.11650295\n",
      "Epoch = 132 : Training Loss = 0.11581204\n",
      "Epoch = 133 : Training Loss = 0.11512992\n",
      "Epoch = 134 : Training Loss = 0.11445409\n",
      "Epoch = 135 : Training Loss = 0.11378549\n",
      "Epoch = 136 : Training Loss = 0.11312225\n",
      "Epoch = 137 : Training Loss = 0.11246349\n",
      "Epoch = 138 : Training Loss = 0.11181021\n",
      "Epoch = 139 : Training Loss = 0.11116419\n",
      "Epoch = 140 : Training Loss = 0.110525906\n",
      "Epoch = 141 : Training Loss = 0.10989421\n",
      "Epoch = 142 : Training Loss = 0.10926899\n",
      "Epoch = 143 : Training Loss = 0.10865113\n",
      "Epoch = 144 : Training Loss = 0.10803646\n",
      "Epoch = 145 : Training Loss = 0.10742689\n",
      "Epoch = 146 : Training Loss = 0.106824666\n",
      "Epoch = 147 : Training Loss = 0.10622904\n",
      "Epoch = 148 : Training Loss = 0.105637915\n",
      "Epoch = 149 : Training Loss = 0.10505311\n",
      "Epoch = 150 : Training Loss = 0.10447292\n",
      "Epoch = 151 : Training Loss = 0.10389706\n",
      "Epoch = 152 : Training Loss = 0.10332946\n",
      "Epoch = 153 : Training Loss = 0.102771774\n",
      "Epoch = 154 : Training Loss = 0.102216974\n",
      "Epoch = 155 : Training Loss = 0.1016661\n",
      "Epoch = 156 : Training Loss = 0.101121925\n",
      "Epoch = 157 : Training Loss = 0.100585304\n",
      "Epoch = 158 : Training Loss = 0.10005476\n",
      "Epoch = 159 : Training Loss = 0.09952605\n",
      "Epoch = 160 : Training Loss = 0.099000044\n",
      "Epoch = 161 : Training Loss = 0.098480485\n",
      "Epoch = 162 : Training Loss = 0.09796512\n",
      "Epoch = 163 : Training Loss = 0.09745596\n",
      "Epoch = 164 : Training Loss = 0.09695241\n",
      "Epoch = 165 : Training Loss = 0.09645272\n",
      "Epoch = 166 : Training Loss = 0.09595799\n",
      "Epoch = 167 : Training Loss = 0.09546896\n",
      "Epoch = 168 : Training Loss = 0.09498806\n",
      "Epoch = 169 : Training Loss = 0.09451415\n",
      "Epoch = 170 : Training Loss = 0.09403774\n",
      "Epoch = 171 : Training Loss = 0.09357048\n",
      "Epoch = 172 : Training Loss = 0.09309777\n",
      "Epoch = 173 : Training Loss = 0.09263423\n",
      "Epoch = 174 : Training Loss = 0.09218215\n",
      "Epoch = 175 : Training Loss = 0.09173449\n",
      "Epoch = 176 : Training Loss = 0.09128916\n",
      "Epoch = 177 : Training Loss = 0.09083665\n",
      "Epoch = 178 : Training Loss = 0.09038771\n",
      "Epoch = 179 : Training Loss = 0.089948095\n",
      "Epoch = 180 : Training Loss = 0.089518964\n",
      "Epoch = 181 : Training Loss = 0.08909642\n",
      "Epoch = 182 : Training Loss = 0.08867675\n",
      "Epoch = 183 : Training Loss = 0.08825332\n",
      "Epoch = 184 : Training Loss = 0.08782824\n",
      "Epoch = 185 : Training Loss = 0.08740952\n",
      "Epoch = 186 : Training Loss = 0.08699782\n",
      "Epoch = 187 : Training Loss = 0.08659495\n",
      "Epoch = 188 : Training Loss = 0.08620133\n",
      "Epoch = 189 : Training Loss = 0.08580291\n",
      "Epoch = 190 : Training Loss = 0.085401796\n",
      "Epoch = 191 : Training Loss = 0.085005306\n",
      "Epoch = 192 : Training Loss = 0.08461663\n",
      "Epoch = 193 : Training Loss = 0.08423742\n",
      "Epoch = 194 : Training Loss = 0.08385631\n",
      "Epoch = 195 : Training Loss = 0.08347052\n",
      "Epoch = 196 : Training Loss = 0.08308791\n",
      "Epoch = 197 : Training Loss = 0.082717955\n",
      "Epoch = 198 : Training Loss = 0.08236168\n",
      "Epoch = 199 : Training Loss = 0.08199346\n",
      "Epoch = 200 : Training Loss = 0.08161647\n",
      "Epoch = 201 : Training Loss = 0.08124421\n",
      "Epoch = 202 : Training Loss = 0.080888025\n",
      "Epoch = 203 : Training Loss = 0.08053788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 204 : Training Loss = 0.080186896\n",
      "Epoch = 205 : Training Loss = 0.079827115\n",
      "Epoch = 206 : Training Loss = 0.0794678\n",
      "Epoch = 207 : Training Loss = 0.07911778\n",
      "Epoch = 208 : Training Loss = 0.07878119\n",
      "Epoch = 209 : Training Loss = 0.07844476\n",
      "Epoch = 210 : Training Loss = 0.0781028\n",
      "Epoch = 211 : Training Loss = 0.07775454\n",
      "Epoch = 212 : Training Loss = 0.077414945\n",
      "Epoch = 213 : Training Loss = 0.07708011\n",
      "Epoch = 214 : Training Loss = 0.0767496\n",
      "Epoch = 215 : Training Loss = 0.07642406\n",
      "Epoch = 216 : Training Loss = 0.07610861\n",
      "Epoch = 217 : Training Loss = 0.07579544\n",
      "Epoch = 218 : Training Loss = 0.07547094\n",
      "Epoch = 219 : Training Loss = 0.075139746\n",
      "Epoch = 220 : Training Loss = 0.07481698\n",
      "Epoch = 221 : Training Loss = 0.07450205\n",
      "Epoch = 222 : Training Loss = 0.07419294\n",
      "Epoch = 223 : Training Loss = 0.07388949\n",
      "Epoch = 224 : Training Loss = 0.07358592\n",
      "Epoch = 225 : Training Loss = 0.07327749\n",
      "Epoch = 226 : Training Loss = 0.0729657\n",
      "Epoch = 227 : Training Loss = 0.072655484\n",
      "Epoch = 228 : Training Loss = 0.07235093\n",
      "Epoch = 229 : Training Loss = 0.07205246\n",
      "Epoch = 230 : Training Loss = 0.071756445\n",
      "Epoch = 231 : Training Loss = 0.07146697\n",
      "Epoch = 232 : Training Loss = 0.07117882\n",
      "Epoch = 233 : Training Loss = 0.07088875\n",
      "Epoch = 234 : Training Loss = 0.07059259\n",
      "Epoch = 235 : Training Loss = 0.070296615\n",
      "Epoch = 236 : Training Loss = 0.07000436\n",
      "Epoch = 237 : Training Loss = 0.06971812\n",
      "Epoch = 238 : Training Loss = 0.06944614\n",
      "Epoch = 239 : Training Loss = 0.06917982\n",
      "Epoch = 240 : Training Loss = 0.06890366\n",
      "Epoch = 241 : Training Loss = 0.06860727\n",
      "Epoch = 242 : Training Loss = 0.068317756\n",
      "Epoch = 243 : Training Loss = 0.068036884\n",
      "Epoch = 244 : Training Loss = 0.067760944\n",
      "Epoch = 245 : Training Loss = 0.067489706\n",
      "Epoch = 246 : Training Loss = 0.067229226\n",
      "Epoch = 247 : Training Loss = 0.06697291\n",
      "Epoch = 248 : Training Loss = 0.0667076\n",
      "Epoch = 249 : Training Loss = 0.06644271\n",
      "Epoch = 250 : Training Loss = 0.06616086\n",
      "Epoch = 251 : Training Loss = 0.065884724\n",
      "Epoch = 252 : Training Loss = 0.06561604\n",
      "Epoch = 253 : Training Loss = 0.06535371\n",
      "Epoch = 254 : Training Loss = 0.06509669\n",
      "Epoch = 255 : Training Loss = 0.06484802\n",
      "Epoch = 256 : Training Loss = 0.064604886\n",
      "Epoch = 257 : Training Loss = 0.064353466\n",
      "Epoch = 258 : Training Loss = 0.064101085\n",
      "Epoch = 259 : Training Loss = 0.063836224\n",
      "Epoch = 260 : Training Loss = 0.06357201\n",
      "Epoch = 261 : Training Loss = 0.0633161\n",
      "Epoch = 262 : Training Loss = 0.06306754\n",
      "Epoch = 263 : Training Loss = 0.06282827\n",
      "Epoch = 264 : Training Loss = 0.062585875\n",
      "Epoch = 265 : Training Loss = 0.062358364\n",
      "Epoch = 266 : Training Loss = 0.062110808\n",
      "Epoch = 267 : Training Loss = 0.06185665\n",
      "Epoch = 268 : Training Loss = 0.06159235\n",
      "Epoch = 269 : Training Loss = 0.061335616\n",
      "Epoch = 270 : Training Loss = 0.061091993\n",
      "Epoch = 271 : Training Loss = 0.06086316\n",
      "Epoch = 272 : Training Loss = 0.060645938\n",
      "Epoch = 273 : Training Loss = 0.060430318\n",
      "Epoch = 274 : Training Loss = 0.06021192\n",
      "Epoch = 275 : Training Loss = 0.059954457\n",
      "Epoch = 276 : Training Loss = 0.059696052\n",
      "Epoch = 277 : Training Loss = 0.0594407\n",
      "Epoch = 278 : Training Loss = 0.05919809\n",
      "Epoch = 279 : Training Loss = 0.058966633\n",
      "Epoch = 280 : Training Loss = 0.058742464\n",
      "Epoch = 281 : Training Loss = 0.05852597\n",
      "Epoch = 282 : Training Loss = 0.05830798\n",
      "Epoch = 283 : Training Loss = 0.05810319\n",
      "Epoch = 284 : Training Loss = 0.05787843\n",
      "Epoch = 285 : Training Loss = 0.05764997\n",
      "Epoch = 286 : Training Loss = 0.057404276\n",
      "Epoch = 287 : Training Loss = 0.05716538\n",
      "Epoch = 288 : Training Loss = 0.056928985\n",
      "Epoch = 289 : Training Loss = 0.056702018\n",
      "Epoch = 290 : Training Loss = 0.056476887\n",
      "Epoch = 291 : Training Loss = 0.056252353\n",
      "Epoch = 292 : Training Loss = 0.0560326\n",
      "Epoch = 293 : Training Loss = 0.055819858\n",
      "Epoch = 294 : Training Loss = 0.055625375\n",
      "Epoch = 295 : Training Loss = 0.055425733\n",
      "Epoch = 296 : Training Loss = 0.055245474\n",
      "Epoch = 297 : Training Loss = 0.055034954\n",
      "Epoch = 298 : Training Loss = 0.054820392\n",
      "Epoch = 299 : Training Loss = 0.054602023\n",
      "Epoch = 300 : Training Loss = 0.05438034\n",
      "Epoch = 301 : Training Loss = 0.05414873\n",
      "Epoch = 302 : Training Loss = 0.05390992\n",
      "Epoch = 303 : Training Loss = 0.053676963\n",
      "Epoch = 304 : Training Loss = 0.05345968\n",
      "Epoch = 305 : Training Loss = 0.053257838\n",
      "Epoch = 306 : Training Loss = 0.05306865\n",
      "Epoch = 307 : Training Loss = 0.052886937\n",
      "Epoch = 308 : Training Loss = 0.05271546\n",
      "Epoch = 309 : Training Loss = 0.052562255\n",
      "Epoch = 310 : Training Loss = 0.052392453\n",
      "Epoch = 311 : Training Loss = 0.05222858\n",
      "Epoch = 312 : Training Loss = 0.051979996\n",
      "Epoch = 313 : Training Loss = 0.051732264\n",
      "Epoch = 314 : Training Loss = 0.051488347\n",
      "Epoch = 315 : Training Loss = 0.051257014\n",
      "Epoch = 316 : Training Loss = 0.05103285\n",
      "Epoch = 317 : Training Loss = 0.050824612\n",
      "Epoch = 318 : Training Loss = 0.050648566\n",
      "Epoch = 319 : Training Loss = 0.050478928\n",
      "Epoch = 320 : Training Loss = 0.050339524\n",
      "Epoch = 321 : Training Loss = 0.050184958\n",
      "Epoch = 322 : Training Loss = 0.050017487\n",
      "Epoch = 323 : Training Loss = 0.049821157\n",
      "Epoch = 324 : Training Loss = 0.0495935\n",
      "Epoch = 325 : Training Loss = 0.049346752\n",
      "Epoch = 326 : Training Loss = 0.04910724\n",
      "Epoch = 327 : Training Loss = 0.048883896\n",
      "Epoch = 328 : Training Loss = 0.048689764\n",
      "Epoch = 329 : Training Loss = 0.04850248\n",
      "Epoch = 330 : Training Loss = 0.048326027\n",
      "Epoch = 331 : Training Loss = 0.048147205\n",
      "Epoch = 332 : Training Loss = 0.047962997\n",
      "Epoch = 333 : Training Loss = 0.04777422\n",
      "Epoch = 334 : Training Loss = 0.04758273\n",
      "Epoch = 335 : Training Loss = 0.04738935\n",
      "Epoch = 336 : Training Loss = 0.0472125\n",
      "Epoch = 337 : Training Loss = 0.047062207\n",
      "Epoch = 338 : Training Loss = 0.046879176\n",
      "Epoch = 339 : Training Loss = 0.046710547\n",
      "Epoch = 340 : Training Loss = 0.04653323\n",
      "Epoch = 341 : Training Loss = 0.04637774\n",
      "Epoch = 342 : Training Loss = 0.04623371\n",
      "Epoch = 343 : Training Loss = 0.046095207\n",
      "Epoch = 344 : Training Loss = 0.04595716\n",
      "Epoch = 345 : Training Loss = 0.04580737\n",
      "Epoch = 346 : Training Loss = 0.04562264\n",
      "Epoch = 347 : Training Loss = 0.04542683\n",
      "Epoch = 348 : Training Loss = 0.045194253\n",
      "Epoch = 349 : Training Loss = 0.04496841\n",
      "Epoch = 350 : Training Loss = 0.04472548\n",
      "Epoch = 351 : Training Loss = 0.04451665\n",
      "Epoch = 352 : Training Loss = 0.04432664\n",
      "Epoch = 353 : Training Loss = 0.044170085\n",
      "Epoch = 354 : Training Loss = 0.044037968\n",
      "Epoch = 355 : Training Loss = 0.04390938\n",
      "Epoch = 356 : Training Loss = 0.043779116\n",
      "Epoch = 357 : Training Loss = 0.043600224\n",
      "Epoch = 358 : Training Loss = 0.04342182\n",
      "Epoch = 359 : Training Loss = 0.043233253\n",
      "Epoch = 360 : Training Loss = 0.04304558\n",
      "Epoch = 361 : Training Loss = 0.042857938\n",
      "Epoch = 362 : Training Loss = 0.042672183\n",
      "Epoch = 363 : Training Loss = 0.04248694\n",
      "Epoch = 364 : Training Loss = 0.042314656\n",
      "Epoch = 365 : Training Loss = 0.042149547\n",
      "Epoch = 366 : Training Loss = 0.041997038\n",
      "Epoch = 367 : Training Loss = 0.041832943\n",
      "Epoch = 368 : Training Loss = 0.041667026\n",
      "Epoch = 369 : Training Loss = 0.04149077\n",
      "Epoch = 370 : Training Loss = 0.041321233\n",
      "Epoch = 371 : Training Loss = 0.041152917\n",
      "Epoch = 372 : Training Loss = 0.040997498\n",
      "Epoch = 373 : Training Loss = 0.040853847\n",
      "Epoch = 374 : Training Loss = 0.040731627\n",
      "Epoch = 375 : Training Loss = 0.040640235\n",
      "Epoch = 376 : Training Loss = 0.04056915\n",
      "Epoch = 377 : Training Loss = 0.040510155\n",
      "Epoch = 378 : Training Loss = 0.040432487\n",
      "Epoch = 379 : Training Loss = 0.04035402\n",
      "Epoch = 380 : Training Loss = 0.040265463\n",
      "Epoch = 381 : Training Loss = 0.04016277\n",
      "Epoch = 382 : Training Loss = 0.04004339\n",
      "Epoch = 383 : Training Loss = 0.039802503\n",
      "Epoch = 384 : Training Loss = 0.03946778\n",
      "Epoch = 385 : Training Loss = 0.039113738\n",
      "Epoch = 386 : Training Loss = 0.03886263\n",
      "Epoch = 387 : Training Loss = 0.038758848\n",
      "Epoch = 388 : Training Loss = 0.038730554\n",
      "Epoch = 389 : Training Loss = 0.038686864\n",
      "Epoch = 390 : Training Loss = 0.03856505\n",
      "Epoch = 391 : Training Loss = 0.038383216\n",
      "Epoch = 392 : Training Loss = 0.038182173\n",
      "Epoch = 393 : Training Loss = 0.03800551\n",
      "Epoch = 394 : Training Loss = 0.037833292\n",
      "Epoch = 395 : Training Loss = 0.037644606\n",
      "Epoch = 396 : Training Loss = 0.037462626\n",
      "Epoch = 397 : Training Loss = 0.037288077\n",
      "Epoch = 398 : Training Loss = 0.03717093\n",
      "Epoch = 399 : Training Loss = 0.037090138\n",
      "Epoch = 400 : Training Loss = 0.037017997\n",
      "Epoch = 401 : Training Loss = 0.036928877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 402 : Training Loss = 0.03677209\n",
      "Epoch = 403 : Training Loss = 0.036577653\n",
      "Epoch = 404 : Training Loss = 0.036394503\n",
      "Epoch = 405 : Training Loss = 0.03623256\n",
      "Epoch = 406 : Training Loss = 0.036105543\n",
      "Epoch = 407 : Training Loss = 0.0359674\n",
      "Epoch = 408 : Training Loss = 0.035827354\n",
      "Epoch = 409 : Training Loss = 0.0356673\n",
      "Epoch = 410 : Training Loss = 0.035508104\n",
      "Epoch = 411 : Training Loss = 0.035358876\n",
      "Epoch = 412 : Training Loss = 0.03521426\n",
      "Epoch = 413 : Training Loss = 0.03508406\n",
      "Epoch = 414 : Training Loss = 0.034964766\n",
      "Epoch = 415 : Training Loss = 0.034862265\n",
      "Epoch = 416 : Training Loss = 0.034750044\n",
      "Epoch = 417 : Training Loss = 0.034638975\n",
      "Epoch = 418 : Training Loss = 0.034511015\n",
      "Epoch = 419 : Training Loss = 0.034379743\n",
      "Epoch = 420 : Training Loss = 0.03422829\n",
      "Epoch = 421 : Training Loss = 0.03408217\n",
      "Epoch = 422 : Training Loss = 0.033951126\n",
      "Epoch = 423 : Training Loss = 0.03384076\n",
      "Epoch = 424 : Training Loss = 0.033764765\n",
      "Epoch = 425 : Training Loss = 0.033737976\n",
      "Epoch = 426 : Training Loss = 0.033773284\n",
      "Epoch = 427 : Training Loss = 0.033869665\n",
      "Epoch = 428 : Training Loss = 0.03400918\n",
      "Epoch = 429 : Training Loss = 0.034115247\n",
      "Epoch = 430 : Training Loss = 0.034131452\n",
      "Epoch = 431 : Training Loss = 0.03389743\n",
      "Epoch = 432 : Training Loss = 0.033488806\n",
      "Epoch = 433 : Training Loss = 0.032972295\n",
      "Epoch = 434 : Training Loss = 0.0325682\n",
      "Epoch = 435 : Training Loss = 0.032444373\n",
      "Epoch = 436 : Training Loss = 0.03252611\n",
      "Epoch = 437 : Training Loss = 0.032624405\n",
      "Epoch = 438 : Training Loss = 0.03260581\n",
      "Epoch = 439 : Training Loss = 0.032349665\n",
      "Epoch = 440 : Training Loss = 0.031962138\n",
      "Epoch = 441 : Training Loss = 0.031675052\n",
      "Epoch = 442 : Training Loss = 0.031583272\n",
      "Epoch = 443 : Training Loss = 0.03159908\n",
      "Epoch = 444 : Training Loss = 0.03161108\n",
      "Epoch = 445 : Training Loss = 0.03154188\n",
      "Epoch = 446 : Training Loss = 0.03133761\n",
      "Epoch = 447 : Training Loss = 0.031068644\n",
      "Epoch = 448 : Training Loss = 0.03086447\n",
      "Epoch = 449 : Training Loss = 0.030741304\n",
      "Epoch = 450 : Training Loss = 0.030701673\n",
      "Epoch = 451 : Training Loss = 0.030671488\n",
      "Epoch = 452 : Training Loss = 0.030584306\n",
      "Epoch = 453 : Training Loss = 0.03043459\n",
      "Epoch = 454 : Training Loss = 0.030261373\n",
      "Epoch = 455 : Training Loss = 0.03009056\n",
      "Epoch = 456 : Training Loss = 0.029950574\n",
      "Epoch = 457 : Training Loss = 0.02985981\n",
      "Epoch = 458 : Training Loss = 0.029783446\n",
      "Epoch = 459 : Training Loss = 0.029699419\n",
      "Epoch = 460 : Training Loss = 0.029603178\n",
      "Epoch = 461 : Training Loss = 0.029494088\n",
      "Epoch = 462 : Training Loss = 0.029369485\n",
      "Epoch = 463 : Training Loss = 0.029235562\n",
      "Epoch = 464 : Training Loss = 0.029106017\n",
      "Epoch = 465 : Training Loss = 0.0289941\n",
      "Epoch = 466 : Training Loss = 0.02890006\n",
      "Epoch = 467 : Training Loss = 0.028825786\n",
      "Epoch = 468 : Training Loss = 0.028746147\n",
      "Epoch = 469 : Training Loss = 0.028637916\n",
      "Epoch = 470 : Training Loss = 0.028500566\n",
      "Epoch = 471 : Training Loss = 0.028366646\n",
      "Epoch = 472 : Training Loss = 0.02822809\n",
      "Epoch = 473 : Training Loss = 0.028121755\n",
      "Epoch = 474 : Training Loss = 0.028025609\n",
      "Epoch = 475 : Training Loss = 0.027936678\n",
      "Epoch = 476 : Training Loss = 0.027868595\n",
      "Epoch = 477 : Training Loss = 0.027791446\n",
      "Epoch = 478 : Training Loss = 0.027698476\n",
      "Epoch = 479 : Training Loss = 0.027609903\n",
      "Epoch = 480 : Training Loss = 0.02751228\n",
      "Epoch = 481 : Training Loss = 0.027395628\n",
      "Epoch = 482 : Training Loss = 0.027292233\n",
      "Epoch = 483 : Training Loss = 0.027199337\n",
      "Epoch = 484 : Training Loss = 0.02710826\n",
      "Epoch = 485 : Training Loss = 0.027046503\n",
      "Epoch = 486 : Training Loss = 0.026999915\n",
      "Epoch = 487 : Training Loss = 0.026893608\n",
      "Epoch = 488 : Training Loss = 0.026763318\n",
      "Epoch = 489 : Training Loss = 0.026639346\n",
      "Epoch = 490 : Training Loss = 0.026488675\n",
      "Epoch = 491 : Training Loss = 0.02636949\n",
      "Epoch = 492 : Training Loss = 0.026245568\n",
      "Epoch = 493 : Training Loss = 0.026126496\n",
      "Epoch = 494 : Training Loss = 0.026014596\n",
      "Epoch = 495 : Training Loss = 0.025933258\n",
      "Epoch = 496 : Training Loss = 0.02585632\n",
      "Epoch = 497 : Training Loss = 0.025760604\n",
      "Epoch = 498 : Training Loss = 0.025665507\n",
      "Epoch = 499 : Training Loss = 0.025566895\n",
      "Epoch = 500 : Training Loss = 0.025477417\n"
     ]
    }
   ],
   "source": [
    " for epoch in 1:epochs \n",
    "    train!(loss , ps, [(X_train , y_train)],opt)\n",
    "    train_loss=loss(X_train , y_train)\n",
    "    push!(loss_history,train_loss)\n",
    "    println(\"Epoch = $epoch : Training Loss = $train_loss\")\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f235324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10000 Matrix{Float32}:\n",
       " 1.87637f-7   1.01691f-8   2.49791f-7   …  5.78494f-11  2.28274f-9\n",
       " 1.21398f-11  9.14f-10     0.998471        4.84441f-12  9.82268f-17\n",
       " 3.63374f-7   1.0          0.000574046     1.89234f-17  8.65572f-10\n",
       " 0.000643829  6.11768f-10  4.38443f-5      3.78447f-15  3.75766f-12\n",
       " 7.05055f-13  1.1654f-26   2.65378f-6      8.77058f-13  6.55432f-8\n",
       " 3.09814f-8   1.45085f-12  2.02222f-6   …  1.0          5.11425f-9\n",
       " 3.38775f-17  1.4172f-11   1.17503f-7      1.13116f-10  1.0\n",
       " 0.999354     1.02255f-21  8.26745f-5      3.01476f-13  1.72456f-16\n",
       " 1.11512f-6   4.15718f-13  0.000823466     4.70602f-7   8.3228f-13\n",
       " 3.00981f-7   2.91727f-21  2.49464f-8      1.88175f-13  4.70849f-11"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_raw =model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a5b8f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000-element Vector{Int64}:\n",
       " 7\n",
       " 2\n",
       " 1\n",
       " 0\n",
       " 4\n",
       " 1\n",
       " 4\n",
       " 9\n",
       " 4\n",
       " 9\n",
       " 0\n",
       " 6\n",
       " 9\n",
       " ⋮\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 8\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat=onecold(y_hat_raw) .-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bfc7c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000-element Vector{Int64}:\n",
       " 7\n",
       " 2\n",
       " 1\n",
       " 0\n",
       " 4\n",
       " 1\n",
       " 4\n",
       " 9\n",
       " 5\n",
       " 9\n",
       " 0\n",
       " 6\n",
       " 9\n",
       " ⋮\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=y_test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "005b4382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(y_hat .==y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd0664ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500-element Vector{Any}:\n",
       " 0.7877831f0\n",
       " 0.67078894f0\n",
       " 0.5911647f0\n",
       " 0.5364811f0\n",
       " 0.49772683f0\n",
       " 0.4683123f0\n",
       " 0.44480518f0\n",
       " 0.42594695f0\n",
       " 0.41077128f0\n",
       " 0.3963644f0\n",
       " 0.38463184f0\n",
       " 0.3747828f0\n",
       " 0.36559424f0\n",
       " ⋮\n",
       " 0.026639346f0\n",
       " 0.026488675f0\n",
       " 0.02636949f0\n",
       " 0.026245568f0\n",
       " 0.026126496f0\n",
       " 0.026014596f0\n",
       " 0.025933258f0\n",
       " 0.02585632f0\n",
       " 0.025760604f0\n",
       " 0.025665507f0\n",
       " 0.025566895f0\n",
       " 0.025477417f0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popfirst!(loss_history)\n",
    "loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e725af4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"600\" viewBox=\"0 0 2400 2400\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip510\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"2400\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip510)\" d=\"\n",
       "M0 2400 L2400 2400 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip511\">\n",
       "    <rect x=\"480\" y=\"240\" width=\"1681\" height=\"1681\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip510)\" d=\"\n",
       "M243.626 2174.14 L2352.76 2174.14 L2352.76 153.712 L243.626 153.712  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip512\">\n",
       "    <rect x=\"243\" y=\"153\" width=\"2110\" height=\"2021\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  299.331,2174.14 299.331,153.712 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  698.078,2174.14 698.078,153.712 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1096.82,2174.14 1096.82,153.712 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1495.57,2174.14 1495.57,153.712 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1894.32,2174.14 1894.32,153.712 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2293.06,2174.14 2293.06,153.712 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  243.626,2174.14 2352.76,2174.14 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  299.331,2174.14 299.331,2155.24 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  698.078,2174.14 698.078,2155.24 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1096.82,2174.14 1096.82,2155.24 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1495.57,2174.14 1495.57,2155.24 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1894.32,2174.14 1894.32,2155.24 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2293.06,2174.14 2293.06,2155.24 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip510)\" d=\"M299.331 2205.06 Q295.72 2205.06 293.891 2208.62 Q292.086 2212.16 292.086 2219.29 Q292.086 2226.4 293.891 2229.97 Q295.72 2233.51 299.331 2233.51 Q302.965 2233.51 304.771 2229.97 Q306.6 2226.4 306.6 2219.29 Q306.6 2212.16 304.771 2208.62 Q302.965 2205.06 299.331 2205.06 M299.331 2201.35 Q305.141 2201.35 308.197 2205.96 Q311.276 2210.54 311.276 2219.29 Q311.276 2228.02 308.197 2232.63 Q305.141 2237.21 299.331 2237.21 Q293.521 2237.21 290.442 2232.63 Q287.387 2228.02 287.387 2219.29 Q287.387 2210.54 290.442 2205.96 Q293.521 2201.35 299.331 2201.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M657.684 2232.6 L665.323 2232.6 L665.323 2206.24 L657.013 2207.91 L657.013 2203.65 L665.277 2201.98 L669.953 2201.98 L669.953 2232.6 L677.592 2232.6 L677.592 2236.54 L657.684 2236.54 L657.684 2232.6 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M697.036 2205.06 Q693.425 2205.06 691.596 2208.62 Q689.791 2212.16 689.791 2219.29 Q689.791 2226.4 691.596 2229.97 Q693.425 2233.51 697.036 2233.51 Q700.67 2233.51 702.476 2229.97 Q704.304 2226.4 704.304 2219.29 Q704.304 2212.16 702.476 2208.62 Q700.67 2205.06 697.036 2205.06 M697.036 2201.35 Q702.846 2201.35 705.902 2205.96 Q708.98 2210.54 708.98 2219.29 Q708.98 2228.02 705.902 2232.63 Q702.846 2237.21 697.036 2237.21 Q691.226 2237.21 688.147 2232.63 Q685.092 2228.02 685.092 2219.29 Q685.092 2210.54 688.147 2205.96 Q691.226 2201.35 697.036 2201.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M727.198 2205.06 Q723.587 2205.06 721.758 2208.62 Q719.953 2212.16 719.953 2219.29 Q719.953 2226.4 721.758 2229.97 Q723.587 2233.51 727.198 2233.51 Q730.832 2233.51 732.638 2229.97 Q734.466 2226.4 734.466 2219.29 Q734.466 2212.16 732.638 2208.62 Q730.832 2205.06 727.198 2205.06 M727.198 2201.35 Q733.008 2201.35 736.064 2205.96 Q739.142 2210.54 739.142 2219.29 Q739.142 2228.02 736.064 2232.63 Q733.008 2237.21 727.198 2237.21 Q721.388 2237.21 718.309 2232.63 Q715.253 2228.02 715.253 2219.29 Q715.253 2210.54 718.309 2205.96 Q721.388 2201.35 727.198 2201.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1060.52 2232.6 L1076.84 2232.6 L1076.84 2236.54 L1054.89 2236.54 L1054.89 2232.6 Q1057.55 2229.85 1062.14 2225.22 Q1066.74 2220.57 1067.92 2219.23 Q1070.17 2216.7 1071.05 2214.97 Q1071.95 2213.21 1071.95 2211.52 Q1071.95 2208.76 1070.01 2207.03 Q1068.09 2205.29 1064.98 2205.29 Q1062.78 2205.29 1060.33 2206.05 Q1057.9 2206.82 1055.12 2208.37 L1055.12 2203.65 Q1057.95 2202.51 1060.4 2201.93 Q1062.85 2201.35 1064.89 2201.35 Q1070.26 2201.35 1073.46 2204.04 Q1076.65 2206.73 1076.65 2211.22 Q1076.65 2213.35 1075.84 2215.27 Q1075.05 2217.16 1072.95 2219.76 Q1072.37 2220.43 1069.27 2223.65 Q1066.16 2226.84 1060.52 2232.6 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1096.65 2205.06 Q1093.04 2205.06 1091.21 2208.62 Q1089.41 2212.16 1089.41 2219.29 Q1089.41 2226.4 1091.21 2229.97 Q1093.04 2233.51 1096.65 2233.51 Q1100.28 2233.51 1102.09 2229.97 Q1103.92 2226.4 1103.92 2219.29 Q1103.92 2212.16 1102.09 2208.62 Q1100.28 2205.06 1096.65 2205.06 M1096.65 2201.35 Q1102.46 2201.35 1105.52 2205.96 Q1108.59 2210.54 1108.59 2219.29 Q1108.59 2228.02 1105.52 2232.63 Q1102.46 2237.21 1096.65 2237.21 Q1090.84 2237.21 1087.76 2232.63 Q1084.71 2228.02 1084.71 2219.29 Q1084.71 2210.54 1087.76 2205.96 Q1090.84 2201.35 1096.65 2201.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1126.81 2205.06 Q1123.2 2205.06 1121.37 2208.62 Q1119.57 2212.16 1119.57 2219.29 Q1119.57 2226.4 1121.37 2229.97 Q1123.2 2233.51 1126.81 2233.51 Q1130.45 2233.51 1132.25 2229.97 Q1134.08 2226.4 1134.08 2219.29 Q1134.08 2212.16 1132.25 2208.62 Q1130.45 2205.06 1126.81 2205.06 M1126.81 2201.35 Q1132.62 2201.35 1135.68 2205.96 Q1138.76 2210.54 1138.76 2219.29 Q1138.76 2228.02 1135.68 2232.63 Q1132.62 2237.21 1126.81 2237.21 Q1121 2237.21 1117.92 2232.63 Q1114.87 2228.02 1114.87 2219.29 Q1114.87 2210.54 1117.92 2205.96 Q1121 2201.35 1126.81 2201.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1469.33 2217.91 Q1472.69 2218.62 1474.56 2220.89 Q1476.46 2223.16 1476.46 2226.49 Q1476.46 2231.61 1472.94 2234.41 Q1469.42 2237.21 1462.94 2237.21 Q1460.77 2237.21 1458.45 2236.77 Q1456.16 2236.35 1453.71 2235.5 L1453.71 2230.98 Q1455.65 2232.12 1457.97 2232.7 Q1460.28 2233.28 1462.8 2233.28 Q1467.2 2233.28 1469.49 2231.54 Q1471.81 2229.8 1471.81 2226.49 Q1471.81 2223.44 1469.66 2221.73 Q1467.53 2219.99 1463.71 2219.99 L1459.68 2219.99 L1459.68 2216.15 L1463.89 2216.15 Q1467.34 2216.15 1469.17 2214.78 Q1471 2213.39 1471 2210.8 Q1471 2208.14 1469.1 2206.73 Q1467.23 2205.29 1463.71 2205.29 Q1461.79 2205.29 1459.59 2205.71 Q1457.39 2206.12 1454.75 2207 L1454.75 2202.84 Q1457.41 2202.1 1459.73 2201.73 Q1462.06 2201.35 1464.12 2201.35 Q1469.45 2201.35 1472.55 2203.79 Q1475.65 2206.19 1475.65 2210.31 Q1475.65 2213.18 1474.01 2215.17 Q1472.36 2217.14 1469.33 2217.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1495.33 2205.06 Q1491.72 2205.06 1489.89 2208.62 Q1488.08 2212.16 1488.08 2219.29 Q1488.08 2226.4 1489.89 2229.97 Q1491.72 2233.51 1495.33 2233.51 Q1498.96 2233.51 1500.77 2229.97 Q1502.6 2226.4 1502.6 2219.29 Q1502.6 2212.16 1500.77 2208.62 Q1498.96 2205.06 1495.33 2205.06 M1495.33 2201.35 Q1501.14 2201.35 1504.19 2205.96 Q1507.27 2210.54 1507.27 2219.29 Q1507.27 2228.02 1504.19 2232.63 Q1501.14 2237.21 1495.33 2237.21 Q1489.52 2237.21 1486.44 2232.63 Q1483.38 2228.02 1483.38 2219.29 Q1483.38 2210.54 1486.44 2205.96 Q1489.52 2201.35 1495.33 2201.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1525.49 2205.06 Q1521.88 2205.06 1520.05 2208.62 Q1518.24 2212.16 1518.24 2219.29 Q1518.24 2226.4 1520.05 2229.97 Q1521.88 2233.51 1525.49 2233.51 Q1529.12 2233.51 1530.93 2229.97 Q1532.76 2226.4 1532.76 2219.29 Q1532.76 2212.16 1530.93 2208.62 Q1529.12 2205.06 1525.49 2205.06 M1525.49 2201.35 Q1531.3 2201.35 1534.36 2205.96 Q1537.43 2210.54 1537.43 2219.29 Q1537.43 2228.02 1534.36 2232.63 Q1531.3 2237.21 1525.49 2237.21 Q1519.68 2237.21 1516.6 2232.63 Q1513.55 2228.02 1513.55 2219.29 Q1513.55 2210.54 1516.6 2205.96 Q1519.68 2201.35 1525.49 2201.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1867.41 2206.05 L1855.6 2224.5 L1867.41 2224.5 L1867.41 2206.05 M1866.18 2201.98 L1872.06 2201.98 L1872.06 2224.5 L1876.99 2224.5 L1876.99 2228.39 L1872.06 2228.39 L1872.06 2236.54 L1867.41 2236.54 L1867.41 2228.39 L1851.81 2228.39 L1851.81 2223.88 L1866.18 2201.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1894.72 2205.06 Q1891.11 2205.06 1889.28 2208.62 Q1887.48 2212.16 1887.48 2219.29 Q1887.48 2226.4 1889.28 2229.97 Q1891.11 2233.51 1894.72 2233.51 Q1898.36 2233.51 1900.16 2229.97 Q1901.99 2226.4 1901.99 2219.29 Q1901.99 2212.16 1900.16 2208.62 Q1898.36 2205.06 1894.72 2205.06 M1894.72 2201.35 Q1900.53 2201.35 1903.59 2205.96 Q1906.67 2210.54 1906.67 2219.29 Q1906.67 2228.02 1903.59 2232.63 Q1900.53 2237.21 1894.72 2237.21 Q1888.91 2237.21 1885.83 2232.63 Q1882.78 2228.02 1882.78 2219.29 Q1882.78 2210.54 1885.83 2205.96 Q1888.91 2201.35 1894.72 2201.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1924.88 2205.06 Q1921.27 2205.06 1919.44 2208.62 Q1917.64 2212.16 1917.64 2219.29 Q1917.64 2226.4 1919.44 2229.97 Q1921.27 2233.51 1924.88 2233.51 Q1928.52 2233.51 1930.32 2229.97 Q1932.15 2226.4 1932.15 2219.29 Q1932.15 2212.16 1930.32 2208.62 Q1928.52 2205.06 1924.88 2205.06 M1924.88 2201.35 Q1930.69 2201.35 1933.75 2205.96 Q1936.83 2210.54 1936.83 2219.29 Q1936.83 2228.02 1933.75 2232.63 Q1930.69 2237.21 1924.88 2237.21 Q1919.07 2237.21 1916 2232.63 Q1912.94 2228.02 1912.94 2219.29 Q1912.94 2210.54 1916 2205.96 Q1919.07 2201.35 1924.88 2201.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M2252.68 2201.98 L2271.04 2201.98 L2271.04 2205.92 L2256.96 2205.92 L2256.96 2214.39 Q2257.98 2214.04 2259 2213.88 Q2260.02 2213.69 2261.04 2213.69 Q2266.83 2213.69 2270.2 2216.86 Q2273.58 2220.04 2273.58 2225.45 Q2273.58 2231.03 2270.11 2234.13 Q2266.64 2237.21 2260.32 2237.21 Q2258.14 2237.21 2255.88 2236.84 Q2253.63 2236.47 2251.22 2235.73 L2251.22 2231.03 Q2253.31 2232.16 2255.53 2232.72 Q2257.75 2233.28 2260.23 2233.28 Q2264.23 2233.28 2266.57 2231.17 Q2268.91 2229.06 2268.91 2225.45 Q2268.91 2221.84 2266.57 2219.73 Q2264.23 2217.63 2260.23 2217.63 Q2258.35 2217.63 2256.48 2218.04 Q2254.63 2218.46 2252.68 2219.34 L2252.68 2201.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M2292.8 2205.06 Q2289.19 2205.06 2287.36 2208.62 Q2285.55 2212.16 2285.55 2219.29 Q2285.55 2226.4 2287.36 2229.97 Q2289.19 2233.51 2292.8 2233.51 Q2296.43 2233.51 2298.24 2229.97 Q2300.07 2226.4 2300.07 2219.29 Q2300.07 2212.16 2298.24 2208.62 Q2296.43 2205.06 2292.8 2205.06 M2292.8 2201.35 Q2298.61 2201.35 2301.66 2205.96 Q2304.74 2210.54 2304.74 2219.29 Q2304.74 2228.02 2301.66 2232.63 Q2298.61 2237.21 2292.8 2237.21 Q2286.99 2237.21 2283.91 2232.63 Q2280.85 2228.02 2280.85 2219.29 Q2280.85 2210.54 2283.91 2205.96 Q2286.99 2201.35 2292.8 2201.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M2322.96 2205.06 Q2319.35 2205.06 2317.52 2208.62 Q2315.71 2212.16 2315.71 2219.29 Q2315.71 2226.4 2317.52 2229.97 Q2319.35 2233.51 2322.96 2233.51 Q2326.59 2233.51 2328.4 2229.97 Q2330.23 2226.4 2330.23 2219.29 Q2330.23 2212.16 2328.4 2208.62 Q2326.59 2205.06 2322.96 2205.06 M2322.96 2201.35 Q2328.77 2201.35 2331.82 2205.96 Q2334.9 2210.54 2334.9 2219.29 Q2334.9 2228.02 2331.82 2232.63 Q2328.77 2237.21 2322.96 2237.21 Q2317.15 2237.21 2314.07 2232.63 Q2311.01 2228.02 2311.01 2219.29 Q2311.01 2210.54 2314.07 2205.96 Q2317.15 2201.35 2322.96 2201.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1186.2 2271.48 L1216.25 2271.48 L1216.25 2276.89 L1192.63 2276.89 L1192.63 2290.96 L1215.26 2290.96 L1215.26 2296.37 L1192.63 2296.37 L1192.63 2313.59 L1216.82 2313.59 L1216.82 2319 L1186.2 2319 L1186.2 2271.48 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1232.8 2313.66 L1232.8 2332.56 L1226.91 2332.56 L1226.91 2283.36 L1232.8 2283.36 L1232.8 2288.77 Q1234.65 2285.58 1237.45 2284.06 Q1240.28 2282.5 1244.19 2282.5 Q1250.69 2282.5 1254.73 2287.65 Q1258.8 2292.81 1258.8 2301.21 Q1258.8 2309.61 1254.73 2314.77 Q1250.69 2319.93 1244.19 2319.93 Q1240.28 2319.93 1237.45 2318.4 Q1234.65 2316.84 1232.8 2313.66 M1252.72 2301.21 Q1252.72 2294.75 1250.05 2291.09 Q1247.41 2287.4 1242.76 2287.4 Q1238.11 2287.4 1235.44 2291.09 Q1232.8 2294.75 1232.8 2301.21 Q1232.8 2307.67 1235.44 2311.36 Q1238.11 2315.03 1242.76 2315.03 Q1247.41 2315.03 1250.05 2311.36 Q1252.72 2307.67 1252.72 2301.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1282.32 2287.46 Q1277.61 2287.46 1274.88 2291.15 Q1272.14 2294.81 1272.14 2301.21 Q1272.14 2307.61 1274.84 2311.3 Q1277.58 2314.96 1282.32 2314.96 Q1287 2314.96 1289.74 2311.27 Q1292.48 2307.58 1292.48 2301.21 Q1292.48 2294.88 1289.74 2291.19 Q1287 2287.46 1282.32 2287.46 M1282.32 2282.5 Q1289.96 2282.5 1294.32 2287.46 Q1298.68 2292.43 1298.68 2301.21 Q1298.68 2309.96 1294.32 2314.96 Q1289.96 2319.93 1282.32 2319.93 Q1274.65 2319.93 1270.29 2314.96 Q1265.96 2309.96 1265.96 2301.21 Q1265.96 2292.43 1270.29 2287.46 Q1274.65 2282.5 1282.32 2282.5 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1334.05 2284.72 L1334.05 2290.2 Q1331.56 2288.83 1329.05 2288.16 Q1326.57 2287.46 1324.02 2287.46 Q1318.32 2287.46 1315.17 2291.09 Q1312.02 2294.69 1312.02 2301.21 Q1312.02 2307.74 1315.17 2311.36 Q1318.32 2314.96 1324.02 2314.96 Q1326.57 2314.96 1329.05 2314.29 Q1331.56 2313.59 1334.05 2312.22 L1334.05 2317.64 Q1331.6 2318.78 1328.95 2319.35 Q1326.34 2319.93 1323.38 2319.93 Q1315.33 2319.93 1310.59 2314.87 Q1305.85 2309.81 1305.85 2301.21 Q1305.85 2292.49 1310.62 2287.49 Q1315.43 2282.5 1323.77 2282.5 Q1326.47 2282.5 1329.05 2283.07 Q1331.63 2283.61 1334.05 2284.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1373.86 2297.49 L1373.86 2319 L1368.01 2319 L1368.01 2297.68 Q1368.01 2292.62 1366.03 2290.1 Q1364.06 2287.59 1360.11 2287.59 Q1355.37 2287.59 1352.63 2290.61 Q1349.9 2293.64 1349.9 2298.86 L1349.9 2319 L1344.01 2319 L1344.01 2269.48 L1349.9 2269.48 L1349.9 2288.89 Q1352 2285.68 1354.83 2284.09 Q1357.69 2282.5 1361.42 2282.5 Q1367.56 2282.5 1370.71 2286.32 Q1373.86 2290.1 1373.86 2297.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1408.27 2284.41 L1408.27 2289.94 Q1405.79 2288.67 1403.11 2288.03 Q1400.44 2287.4 1397.58 2287.4 Q1393.22 2287.4 1391.02 2288.73 Q1388.85 2290.07 1388.85 2292.75 Q1388.85 2294.78 1390.41 2295.96 Q1391.97 2297.11 1396.68 2298.16 L1398.69 2298.6 Q1404.93 2299.94 1407.54 2302.39 Q1410.18 2304.81 1410.18 2309.17 Q1410.18 2314.13 1406.23 2317.03 Q1402.32 2319.93 1395.44 2319.93 Q1392.58 2319.93 1389.46 2319.35 Q1386.37 2318.81 1382.93 2317.7 L1382.93 2311.65 Q1386.18 2313.34 1389.33 2314.2 Q1392.48 2315.03 1395.57 2315.03 Q1399.71 2315.03 1401.94 2313.62 Q1404.16 2312.19 1404.16 2309.61 Q1404.16 2307.23 1402.54 2305.95 Q1400.95 2304.68 1395.51 2303.5 L1393.47 2303.03 Q1388.03 2301.88 1385.61 2299.52 Q1383.19 2297.14 1383.19 2293 Q1383.19 2287.97 1386.75 2285.23 Q1390.32 2282.5 1396.88 2282.5 Q1400.12 2282.5 1402.99 2282.97 Q1405.85 2283.45 1408.27 2284.41 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  243.626,1680.58 2352.76,1680.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  243.626,1180.5 2352.76,1180.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  243.626,680.426 2352.76,680.426 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  243.626,180.347 2352.76,180.347 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  243.626,2174.14 243.626,153.712 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  243.626,1680.58 262.524,1680.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  243.626,1180.5 262.524,1180.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  243.626,680.426 262.524,680.426 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  243.626,180.347 262.524,180.347 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip510)\" d=\"M152.048 1666.38 Q148.437 1666.38 146.608 1669.95 Q144.803 1673.49 144.803 1680.62 Q144.803 1687.72 146.608 1691.29 Q148.437 1694.83 152.048 1694.83 Q155.682 1694.83 157.488 1691.29 Q159.316 1687.72 159.316 1680.62 Q159.316 1673.49 157.488 1669.95 Q155.682 1666.38 152.048 1666.38 M152.048 1662.68 Q157.858 1662.68 160.914 1667.28 Q163.992 1671.87 163.992 1680.62 Q163.992 1689.34 160.914 1693.95 Q157.858 1698.53 152.048 1698.53 Q146.238 1698.53 143.159 1693.95 Q140.103 1689.34 140.103 1680.62 Q140.103 1671.87 143.159 1667.28 Q146.238 1662.68 152.048 1662.68 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M172.21 1691.98 L177.094 1691.98 L177.094 1697.86 L172.21 1697.86 L172.21 1691.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M191.307 1693.93 L207.626 1693.93 L207.626 1697.86 L185.682 1697.86 L185.682 1693.93 Q188.344 1691.17 192.927 1686.54 Q197.534 1681.89 198.714 1680.55 Q200.96 1678.02 201.839 1676.29 Q202.742 1674.53 202.742 1672.84 Q202.742 1670.09 200.798 1668.35 Q198.876 1666.61 195.774 1666.61 Q193.575 1666.61 191.122 1667.38 Q188.691 1668.14 185.913 1669.69 L185.913 1664.97 Q188.737 1663.84 191.191 1663.26 Q193.645 1662.68 195.682 1662.68 Q201.052 1662.68 204.247 1665.36 Q207.441 1668.05 207.441 1672.54 Q207.441 1674.67 206.631 1676.59 Q205.844 1678.49 203.737 1681.08 Q203.159 1681.75 200.057 1684.97 Q196.955 1688.16 191.307 1693.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M149.965 1166.3 Q146.353 1166.3 144.525 1169.87 Q142.719 1173.41 142.719 1180.54 Q142.719 1187.65 144.525 1191.21 Q146.353 1194.75 149.965 1194.75 Q153.599 1194.75 155.404 1191.21 Q157.233 1187.65 157.233 1180.54 Q157.233 1173.41 155.404 1169.87 Q153.599 1166.3 149.965 1166.3 M149.965 1162.6 Q155.775 1162.6 158.83 1167.21 Q161.909 1171.79 161.909 1180.54 Q161.909 1189.27 158.83 1193.87 Q155.775 1198.46 149.965 1198.46 Q144.154 1198.46 141.076 1193.87 Q138.02 1189.27 138.02 1180.54 Q138.02 1171.79 141.076 1167.21 Q144.154 1162.6 149.965 1162.6 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M170.126 1191.9 L175.011 1191.9 L175.011 1197.78 L170.126 1197.78 L170.126 1191.9 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M198.043 1167.3 L186.237 1185.75 L198.043 1185.75 L198.043 1167.3 M196.816 1163.22 L202.696 1163.22 L202.696 1185.75 L207.626 1185.75 L207.626 1189.64 L202.696 1189.64 L202.696 1197.78 L198.043 1197.78 L198.043 1189.64 L182.441 1189.64 L182.441 1185.12 L196.816 1163.22 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M150.289 666.224 Q146.678 666.224 144.849 669.789 Q143.043 673.331 143.043 680.46 Q143.043 687.567 144.849 691.132 Q146.678 694.673 150.289 694.673 Q153.923 694.673 155.728 691.132 Q157.557 687.567 157.557 680.46 Q157.557 673.331 155.728 669.789 Q153.923 666.224 150.289 666.224 M150.289 662.521 Q156.099 662.521 159.154 667.127 Q162.233 671.71 162.233 680.46 Q162.233 689.187 159.154 693.794 Q156.099 698.377 150.289 698.377 Q144.478 698.377 141.4 693.794 Q138.344 689.187 138.344 680.46 Q138.344 671.71 141.4 667.127 Q144.478 662.521 150.289 662.521 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M170.451 691.826 L175.335 691.826 L175.335 697.706 L170.451 697.706 L170.451 691.826 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M196.099 678.562 Q192.95 678.562 191.099 680.715 Q189.27 682.868 189.27 686.618 Q189.27 690.345 191.099 692.52 Q192.95 694.673 196.099 694.673 Q199.247 694.673 201.075 692.52 Q202.927 690.345 202.927 686.618 Q202.927 682.868 201.075 680.715 Q199.247 678.562 196.099 678.562 M205.381 663.909 L205.381 668.169 Q203.622 667.335 201.816 666.896 Q200.034 666.456 198.274 666.456 Q193.645 666.456 191.191 669.581 Q188.761 672.706 188.413 679.025 Q189.779 677.011 191.839 675.946 Q193.899 674.858 196.376 674.858 Q201.585 674.858 204.594 678.03 Q207.626 681.178 207.626 686.618 Q207.626 691.942 204.478 695.159 Q201.33 698.377 196.099 698.377 Q190.103 698.377 186.932 693.794 Q183.761 689.187 183.761 680.46 Q183.761 672.266 187.65 667.405 Q191.538 662.521 198.089 662.521 Q199.849 662.521 201.631 662.868 Q203.436 663.215 205.381 663.909 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M150.543 166.146 Q146.932 166.146 145.103 169.71 Q143.298 173.252 143.298 180.382 Q143.298 187.488 145.103 191.053 Q146.932 194.595 150.543 194.595 Q154.177 194.595 155.983 191.053 Q157.812 187.488 157.812 180.382 Q157.812 173.252 155.983 169.71 Q154.177 166.146 150.543 166.146 M150.543 162.442 Q156.353 162.442 159.409 167.048 Q162.488 171.632 162.488 180.382 Q162.488 189.108 159.409 193.715 Q156.353 198.298 150.543 198.298 Q144.733 198.298 141.654 193.715 Q138.599 189.108 138.599 180.382 Q138.599 171.632 141.654 167.048 Q144.733 162.442 150.543 162.442 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M170.705 191.747 L175.589 191.747 L175.589 197.627 L170.705 197.627 L170.705 191.747 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M195.774 181.215 Q192.441 181.215 190.52 182.997 Q188.622 184.78 188.622 187.905 Q188.622 191.03 190.52 192.812 Q192.441 194.595 195.774 194.595 Q199.108 194.595 201.029 192.812 Q202.95 191.007 202.95 187.905 Q202.95 184.78 201.029 182.997 Q199.131 181.215 195.774 181.215 M191.099 179.224 Q188.089 178.484 186.4 176.423 Q184.733 174.363 184.733 171.4 Q184.733 167.257 187.673 164.849 Q190.636 162.442 195.774 162.442 Q200.936 162.442 203.876 164.849 Q206.816 167.257 206.816 171.4 Q206.816 174.363 205.126 176.423 Q203.46 178.484 200.474 179.224 Q203.853 180.011 205.728 182.303 Q207.626 184.595 207.626 187.905 Q207.626 192.928 204.548 195.613 Q201.492 198.298 195.774 198.298 Q190.057 198.298 186.978 195.613 Q183.923 192.928 183.923 187.905 Q183.923 184.595 185.821 182.303 Q187.719 180.011 191.099 179.224 M189.386 171.84 Q189.386 174.525 191.052 176.03 Q192.742 177.534 195.774 177.534 Q198.784 177.534 200.474 176.03 Q202.186 174.525 202.186 171.84 Q202.186 169.155 200.474 167.65 Q198.784 166.146 195.774 166.146 Q192.742 166.146 191.052 167.65 Q189.386 169.155 189.386 171.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M40.2442 1230.62 L40.2442 1224.19 L82.3533 1224.19 L82.3533 1201.05 L87.7642 1201.05 L87.7642 1230.62 L40.2442 1230.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M56.2221 1181.89 Q56.2221 1186.6 59.9142 1189.34 Q63.5745 1192.08 69.972 1192.08 Q76.3695 1192.08 80.0617 1189.37 Q83.7219 1186.64 83.7219 1181.89 Q83.7219 1177.21 80.0298 1174.48 Q76.3377 1171.74 69.972 1171.74 Q63.6381 1171.74 59.946 1174.48 Q56.2221 1177.21 56.2221 1181.89 M51.2568 1181.89 Q51.2568 1174.25 56.2221 1169.89 Q61.1873 1165.53 69.972 1165.53 Q78.7249 1165.53 83.7219 1169.89 Q88.6872 1174.25 88.6872 1181.89 Q88.6872 1189.56 83.7219 1193.92 Q78.7249 1198.25 69.972 1198.25 Q61.1873 1198.25 56.2221 1193.92 Q51.2568 1189.56 51.2568 1181.89 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M53.1665 1133.1 L58.7047 1133.1 Q57.4316 1135.58 56.795 1138.26 Q56.1584 1140.93 56.1584 1143.79 Q56.1584 1148.15 57.4952 1150.35 Q58.832 1152.52 61.5056 1152.52 Q63.5426 1152.52 64.7203 1150.96 Q65.8661 1149.4 66.9165 1144.69 L67.3621 1142.68 Q68.6989 1136.44 71.1497 1133.83 Q73.5686 1131.19 77.9291 1131.19 Q82.8944 1131.19 85.7908 1135.14 Q88.6872 1139.05 88.6872 1145.93 Q88.6872 1148.79 88.1143 1151.91 Q87.5732 1155 86.4592 1158.44 L80.4118 1158.44 Q82.0987 1155.19 82.958 1152.04 Q83.7856 1148.89 83.7856 1145.8 Q83.7856 1141.66 82.3851 1139.43 Q80.9529 1137.21 78.3747 1137.21 Q75.9876 1137.21 74.7145 1138.83 Q73.4413 1140.42 72.2637 1145.86 L71.7862 1147.9 Q70.6404 1153.34 68.2851 1155.76 Q65.898 1158.18 61.7602 1158.18 Q56.7313 1158.18 53.9941 1154.62 Q51.2568 1151.05 51.2568 1144.49 Q51.2568 1141.25 51.7343 1138.38 Q52.2117 1135.52 53.1665 1133.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M53.1665 1099.14 L58.7047 1099.14 Q57.4316 1101.62 56.795 1104.3 Q56.1584 1106.97 56.1584 1109.83 Q56.1584 1114.19 57.4952 1116.39 Q58.832 1118.55 61.5056 1118.55 Q63.5426 1118.55 64.7203 1116.99 Q65.8661 1115.44 66.9165 1110.72 L67.3621 1108.72 Q68.6989 1102.48 71.1497 1099.87 Q73.5686 1097.23 77.9291 1097.23 Q82.8944 1097.23 85.7908 1101.18 Q88.6872 1105.09 88.6872 1111.97 Q88.6872 1114.83 88.1143 1117.95 Q87.5732 1121.04 86.4592 1124.47 L80.4118 1124.47 Q82.0987 1121.23 82.958 1118.08 Q83.7856 1114.93 83.7856 1111.84 Q83.7856 1107.7 82.3851 1105.47 Q80.9529 1103.24 78.3747 1103.24 Q75.9876 1103.24 74.7145 1104.87 Q73.4413 1106.46 72.2637 1111.9 L71.7862 1113.94 Q70.6404 1119.38 68.2851 1121.8 Q65.898 1124.22 61.7602 1124.22 Q56.7313 1124.22 53.9941 1120.66 Q51.2568 1117.09 51.2568 1110.53 Q51.2568 1107.29 51.7343 1104.42 Q52.2117 1101.56 53.1665 1099.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M973.43 12.096 L981.613 12.096 L981.613 65.6895 L1011.06 65.6895 L1011.06 72.576 L973.43 72.576 L973.43 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1056.68 48.0275 L1056.68 51.6733 L1022.41 51.6733 Q1022.89 59.3701 1027.02 63.421 Q1031.2 67.4314 1038.61 67.4314 Q1042.9 67.4314 1046.91 66.3781 Q1050.96 65.3249 1054.93 63.2184 L1054.93 70.267 Q1050.92 71.9684 1046.71 72.8596 Q1042.5 73.7508 1038.16 73.7508 Q1027.31 73.7508 1020.95 67.4314 Q1014.63 61.1119 1014.63 50.3365 Q1014.63 39.1965 1020.62 32.6746 Q1026.66 26.1121 1036.87 26.1121 Q1046.02 26.1121 1051.33 32.0264 Q1056.68 37.9003 1056.68 48.0275 M1049.22 45.84 Q1049.14 39.7232 1045.78 36.0774 Q1042.46 32.4315 1036.95 32.4315 Q1030.71 32.4315 1026.94 35.9558 Q1023.22 39.4801 1022.65 45.8805 L1049.22 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1089.53 49.7694 Q1080.5 49.7694 1077.01 51.8354 Q1073.53 53.9013 1073.53 58.8839 Q1073.53 62.8538 1076.12 65.2034 Q1078.75 67.5124 1083.25 67.5124 Q1089.45 67.5124 1093.17 63.1374 Q1096.94 58.7219 1096.94 51.4303 L1096.94 49.7694 L1089.53 49.7694 M1104.4 46.6907 L1104.4 72.576 L1096.94 72.576 L1096.94 65.6895 Q1094.39 69.8214 1090.58 71.8063 Q1086.77 73.7508 1081.27 73.7508 Q1074.3 73.7508 1070.17 69.8619 Q1066.07 65.9325 1066.07 59.3701 Q1066.07 51.7138 1071.18 47.825 Q1076.32 43.9361 1086.49 43.9361 L1096.94 43.9361 L1096.94 43.2069 Q1096.94 38.0623 1093.54 35.2672 Q1090.18 32.4315 1084.06 32.4315 Q1080.17 32.4315 1076.49 33.3632 Q1072.8 34.295 1069.4 36.1584 L1069.4 29.2718 Q1073.49 27.692 1077.34 26.9223 Q1081.18 26.1121 1084.83 26.1121 Q1094.67 26.1121 1099.53 31.2163 Q1104.4 36.3204 1104.4 46.6907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1146.04 34.1734 Q1144.78 33.4443 1143.28 33.1202 Q1141.83 32.7556 1140.04 32.7556 Q1133.72 32.7556 1130.32 36.8875 Q1126.96 40.9789 1126.96 48.6757 L1126.96 72.576 L1119.47 72.576 L1119.47 27.2059 L1126.96 27.2059 L1126.96 34.2544 Q1129.31 30.1225 1133.08 28.1376 Q1136.84 26.1121 1142.23 26.1121 Q1143 26.1121 1143.93 26.2337 Q1144.86 26.3147 1146 26.5172 L1146.04 34.1734 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1190.11 45.1919 L1190.11 72.576 L1182.66 72.576 L1182.66 45.4349 Q1182.66 38.994 1180.15 35.7938 Q1177.64 32.5936 1172.61 32.5936 Q1166.58 32.5936 1163.09 36.4419 Q1159.61 40.2903 1159.61 46.9338 L1159.61 72.576 L1152.12 72.576 L1152.12 27.2059 L1159.61 27.2059 L1159.61 34.2544 Q1162.28 30.163 1165.89 28.1376 Q1169.53 26.1121 1174.27 26.1121 Q1182.09 26.1121 1186.1 30.9732 Q1190.11 35.7938 1190.11 45.1919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1204.98 27.2059 L1212.43 27.2059 L1212.43 72.576 L1204.98 72.576 L1204.98 27.2059 M1204.98 9.54393 L1212.43 9.54393 L1212.43 18.9825 L1204.98 18.9825 L1204.98 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1265.74 45.1919 L1265.74 72.576 L1258.29 72.576 L1258.29 45.4349 Q1258.29 38.994 1255.78 35.7938 Q1253.27 32.5936 1248.24 32.5936 Q1242.21 32.5936 1238.72 36.4419 Q1235.24 40.2903 1235.24 46.9338 L1235.24 72.576 L1227.75 72.576 L1227.75 27.2059 L1235.24 27.2059 L1235.24 34.2544 Q1237.91 30.163 1241.52 28.1376 Q1245.16 26.1121 1249.9 26.1121 Q1257.72 26.1121 1261.73 30.9732 Q1265.74 35.7938 1265.74 45.1919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1310.47 49.3643 Q1310.47 41.2625 1307.1 36.8065 Q1303.78 32.3505 1297.75 32.3505 Q1291.75 32.3505 1288.39 36.8065 Q1285.07 41.2625 1285.07 49.3643 Q1285.07 57.4256 1288.39 61.8816 Q1291.75 66.3376 1297.75 66.3376 Q1303.78 66.3376 1307.1 61.8816 Q1310.47 57.4256 1310.47 49.3643 M1317.92 66.9452 Q1317.92 78.5308 1312.77 84.1616 Q1307.63 89.8329 1297.02 89.8329 Q1293.09 89.8329 1289.6 89.2252 Q1286.12 88.6581 1282.84 87.4428 L1282.84 80.1917 Q1286.12 81.9741 1289.32 82.8248 Q1292.52 83.6755 1295.84 83.6755 Q1303.17 83.6755 1306.82 79.8271 Q1310.47 76.0193 1310.47 68.282 L1310.47 64.5957 Q1308.16 68.6061 1304.55 70.5911 Q1300.95 72.576 1295.92 72.576 Q1287.58 72.576 1282.47 66.2161 Q1277.37 59.8562 1277.37 49.3643 Q1277.37 38.832 1282.47 32.472 Q1287.58 26.1121 1295.92 26.1121 Q1300.95 26.1121 1304.55 28.0971 Q1308.16 30.082 1310.47 34.0924 L1310.47 27.2059 L1317.92 27.2059 L1317.92 66.9452 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1405.26 16.7545 L1405.26 25.383 Q1401.12 21.5346 1396.43 19.6307 Q1391.77 17.7268 1386.5 17.7268 Q1376.13 17.7268 1370.62 24.0867 Q1365.11 30.4061 1365.11 42.3968 Q1365.11 54.3469 1370.62 60.7069 Q1376.13 67.0263 1386.5 67.0263 Q1391.77 67.0263 1396.43 65.1223 Q1401.12 63.2184 1405.26 59.3701 L1405.26 67.9175 Q1400.96 70.8341 1396.14 72.2924 Q1391.36 73.7508 1386.01 73.7508 Q1372.28 73.7508 1364.38 65.3654 Q1356.48 56.9395 1356.48 42.3968 Q1356.48 27.8135 1364.38 19.4281 Q1372.28 11.0023 1386.01 11.0023 Q1391.44 11.0023 1396.22 12.4606 Q1401.04 13.8784 1405.26 16.7545 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1416.8 54.671 L1416.8 27.2059 L1424.26 27.2059 L1424.26 54.3874 Q1424.26 60.8284 1426.77 64.0691 Q1429.28 67.2693 1434.3 67.2693 Q1440.34 67.2693 1443.82 63.421 Q1447.35 59.5726 1447.35 52.9291 L1447.35 27.2059 L1454.8 27.2059 L1454.8 72.576 L1447.35 72.576 L1447.35 65.6084 Q1444.63 69.7404 1441.03 71.7658 Q1437.46 73.7508 1432.72 73.7508 Q1424.9 73.7508 1420.85 68.8897 Q1416.8 64.0286 1416.8 54.671 M1435.56 26.1121 L1435.56 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1496.44 34.1734 Q1495.19 33.4443 1493.69 33.1202 Q1492.23 32.7556 1490.45 32.7556 Q1484.13 32.7556 1480.72 36.8875 Q1477.36 40.9789 1477.36 48.6757 L1477.36 72.576 L1469.87 72.576 L1469.87 27.2059 L1477.36 27.2059 L1477.36 34.2544 Q1479.71 30.1225 1483.48 28.1376 Q1487.25 26.1121 1492.63 26.1121 Q1493.4 26.1121 1494.34 26.2337 Q1495.27 26.3147 1496.4 26.5172 L1496.44 34.1734 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1498.91 27.2059 L1506.81 27.2059 L1520.99 65.2844 L1535.17 27.2059 L1543.07 27.2059 L1526.05 72.576 L1515.93 72.576 L1498.91 27.2059 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1592.17 48.0275 L1592.17 51.6733 L1557.89 51.6733 Q1558.38 59.3701 1562.51 63.421 Q1566.69 67.4314 1574.1 67.4314 Q1578.39 67.4314 1582.4 66.3781 Q1586.45 65.3249 1590.42 63.2184 L1590.42 70.267 Q1586.41 71.9684 1582.2 72.8596 Q1577.99 73.7508 1573.65 73.7508 Q1562.8 73.7508 1556.44 67.4314 Q1550.12 61.1119 1550.12 50.3365 Q1550.12 39.1965 1556.11 32.6746 Q1562.15 26.1121 1572.36 26.1121 Q1581.51 26.1121 1586.82 32.0264 Q1592.17 37.9003 1592.17 48.0275 M1584.71 45.84 Q1584.63 39.7232 1581.27 36.0774 Q1577.95 32.4315 1572.44 32.4315 Q1566.2 32.4315 1562.43 35.9558 Q1558.7 39.4801 1558.14 45.8805 L1584.71 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip512)\" style=\"stroke:#0000ff; stroke-linecap:round; stroke-linejoin:round; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  303.319,210.894 307.306,503.425 311.294,702.517 315.281,839.248 319.268,936.149 323.256,1009.7 327.243,1068.47 331.231,1115.63 335.218,1153.57 339.206,1189.59 \n",
       "  343.193,1218.93 347.181,1243.56 351.168,1266.53 355.156,1287.72 359.143,1307.98 363.131,1325.36 367.118,1342.58 371.106,1360.04 375.093,1375.99 379.08,1390.89 \n",
       "  383.068,1404.48 387.055,1417.85 391.043,1430.89 395.03,1443.11 399.018,1455.18 403.005,1466.85 406.993,1478.37 410.98,1489.36 414.968,1499.74 418.955,1509.66 \n",
       "  422.943,1518.97 426.93,1528.06 430.917,1536.68 434.905,1545.01 438.892,1553.14 442.88,1561.01 446.867,1568.59 450.855,1575.74 454.842,1582.74 458.83,1589.45 \n",
       "  462.817,1595.92 466.805,1602.24 470.792,1608.51 474.78,1614.69 478.767,1620.76 482.755,1626.77 486.742,1632.7 490.729,1638.5 494.717,1644.13 498.704,1649.68 \n",
       "  502.692,1655.1 506.679,1660.4 510.667,1665.57 514.654,1670.67 518.642,1675.63 522.629,1680.49 526.617,1685.23 530.604,1689.87 534.592,1694.42 538.579,1698.89 \n",
       "  542.567,1703.27 546.554,1707.57 550.541,1711.78 554.529,1715.92 558.516,1719.98 562.504,1723.96 566.491,1727.87 570.479,1731.69 574.466,1735.45 578.454,1739.15 \n",
       "  582.441,1742.78 586.429,1746.33 590.416,1749.82 594.404,1753.25 598.391,1756.61 602.378,1759.92 606.366,1763.17 610.353,1766.38 614.341,1769.55 618.328,1772.67 \n",
       "  622.316,1775.74 626.303,1778.76 630.291,1781.74 634.278,1784.68 638.266,1787.56 642.253,1790.41 646.241,1793.21 650.228,1795.98 654.216,1798.71 658.203,1801.4 \n",
       "  662.19,1804.06 666.178,1806.69 670.165,1809.29 674.153,1811.86 678.14,1814.41 682.128,1816.94 686.115,1819.44 690.103,1821.91 694.09,1824.34 698.078,1826.75 \n",
       "  702.065,1829.12 706.053,1831.47 710.04,1833.78 714.027,1836.07 718.015,1838.32 722.002,1840.55 725.99,1842.75 729.977,1844.92 733.965,1847.07 737.952,1849.2 \n",
       "  741.94,1851.31 745.927,1853.39 749.915,1855.44 753.902,1857.47 757.89,1859.49 761.877,1861.49 765.865,1863.47 769.852,1865.42 773.839,1867.36 777.827,1869.28 \n",
       "  781.814,1871.19 785.802,1873.08 789.789,1874.95 793.777,1876.8 797.764,1878.64 801.752,1880.46 805.739,1882.27 809.727,1884.07 813.714,1885.85 817.702,1887.61 \n",
       "  821.689,1889.36 825.677,1891.09 829.664,1892.79 833.651,1894.48 837.639,1896.15 841.626,1897.81 845.614,1899.46 849.601,1901.09 853.589,1902.71 857.576,1904.3 \n",
       "  861.564,1905.88 865.551,1907.45 869.539,1908.99 873.526,1910.53 877.514,1912.05 881.501,1913.56 885.488,1915.05 889.476,1916.53 893.463,1917.99 897.451,1919.44 \n",
       "  901.438,1920.88 905.426,1922.3 909.413,1923.69 913.401,1925.08 917.388,1926.46 921.376,1927.82 925.363,1929.16 929.351,1930.49 933.338,1931.81 937.326,1933.12 \n",
       "  941.313,1934.42 945.3,1935.71 949.288,1936.98 953.275,1938.24 957.263,1939.49 961.25,1940.73 965.238,1941.95 969.225,1943.15 973.213,1944.34 977.2,1945.53 \n",
       "  981.188,1946.7 985.175,1947.88 989.163,1949.04 993.15,1950.17 997.137,1951.29 1001.12,1952.4 1005.11,1953.53 1009.1,1954.66 1013.09,1955.76 1017.07,1956.83 \n",
       "  1021.06,1957.89 1025.05,1958.93 1029.04,1959.99 1033.02,1961.06 1037.01,1962.1 1041,1963.13 1044.99,1964.14 1048.97,1965.12 1052.96,1966.12 1056.95,1967.12 \n",
       "  1060.94,1968.11 1064.92,1969.09 1068.91,1970.03 1072.9,1970.99 1076.89,1971.95 1080.87,1972.91 1084.86,1973.83 1088.85,1974.72 1092.84,1975.65 1096.82,1976.59 \n",
       "  1100.81,1977.52 1104.8,1978.41 1108.79,1979.29 1112.77,1980.16 1116.76,1981.06 1120.75,1981.96 1124.74,1982.84 1128.72,1983.68 1132.71,1984.52 1136.7,1985.37 \n",
       "  1140.69,1986.24 1144.67,1987.09 1148.66,1987.93 1152.65,1988.76 1156.64,1989.57 1160.62,1990.36 1164.61,1991.14 1168.6,1991.95 1172.59,1992.78 1176.57,1993.59 \n",
       "  1180.56,1994.38 1184.55,1995.15 1188.54,1995.91 1192.52,1996.67 1196.51,1997.44 1200.5,1998.22 1204.49,1998.99 1208.47,1999.76 1212.46,2000.5 1216.45,2001.24 \n",
       "  1220.44,2001.97 1224.42,2002.69 1228.41,2003.41 1232.4,2004.15 1236.39,2004.89 1240.37,2005.62 1244.36,2006.34 1248.35,2007.02 1252.34,2007.68 1256.32,2008.38 \n",
       "  1260.31,2009.12 1264.3,2009.84 1268.29,2010.54 1272.27,2011.23 1276.26,2011.91 1280.25,2012.56 1284.23,2013.2 1288.22,2013.87 1292.21,2014.53 1296.2,2015.23 \n",
       "  1300.18,2015.92 1304.17,2016.6 1308.16,2017.25 1312.15,2017.89 1316.13,2018.52 1320.12,2019.12 1324.11,2019.75 1328.1,2020.38 1332.08,2021.05 1336.07,2021.71 \n",
       "  1340.06,2022.35 1344.05,2022.97 1348.03,2023.57 1352.02,2024.17 1356.01,2024.74 1360,2025.36 1363.98,2026 1367.97,2026.66 1371.96,2027.3 1375.95,2027.91 \n",
       "  1379.93,2028.48 1383.92,2029.02 1387.91,2029.56 1391.9,2030.11 1395.88,2030.75 1399.87,2031.4 1403.86,2032.04 1407.85,2032.64 1411.83,2033.22 1415.82,2033.78 \n",
       "  1419.81,2034.32 1423.8,2034.87 1427.78,2035.38 1431.77,2035.94 1435.76,2036.51 1439.75,2037.13 1443.73,2037.73 1447.72,2038.32 1451.71,2038.88 1455.7,2039.45 \n",
       "  1459.68,2040.01 1463.67,2040.56 1467.66,2041.09 1471.65,2041.58 1475.63,2042.08 1479.62,2042.53 1483.61,2043.05 1487.6,2043.59 1491.58,2044.13 1495.57,2044.69 \n",
       "  1499.56,2045.27 1503.55,2045.87 1507.53,2046.45 1511.52,2046.99 1515.51,2047.5 1519.5,2047.97 1523.48,2048.42 1527.47,2048.85 1531.46,2049.24 1535.45,2049.66 \n",
       "  1539.43,2050.07 1543.42,2050.69 1547.41,2051.31 1551.4,2051.92 1555.38,2052.5 1559.37,2053.06 1563.36,2053.58 1567.34,2054.02 1571.33,2054.44 1575.32,2054.79 \n",
       "  1579.31,2055.18 1583.29,2055.6 1587.28,2056.09 1591.27,2056.66 1595.26,2057.28 1599.24,2057.87 1603.23,2058.43 1607.22,2058.92 1611.21,2059.39 1615.19,2059.83 \n",
       "  1619.18,2060.27 1623.17,2060.74 1627.16,2061.21 1631.14,2061.69 1635.13,2062.17 1639.12,2062.61 1643.11,2062.99 1647.09,2063.45 1651.08,2063.87 1655.07,2064.31 \n",
       "  1659.06,2064.7 1663.04,2065.06 1667.03,2065.41 1671.02,2065.75 1675.01,2066.13 1678.99,2066.59 1682.98,2067.08 1686.97,2067.66 1690.96,2068.22 1694.94,2068.83 \n",
       "  1698.93,2069.35 1702.92,2069.83 1706.91,2070.22 1710.89,2070.55 1714.88,2070.87 1718.87,2071.2 1722.86,2071.64 1726.84,2072.09 1730.83,2072.56 1734.82,2073.03 \n",
       "  1738.81,2073.5 1742.79,2073.96 1746.78,2074.43 1750.77,2074.86 1754.76,2075.27 1758.74,2075.65 1762.73,2076.06 1766.72,2076.48 1770.71,2076.92 1774.69,2077.34 \n",
       "  1778.68,2077.76 1782.67,2078.15 1786.66,2078.51 1790.64,2078.82 1794.63,2079.04 1798.62,2079.22 1802.61,2079.37 1806.59,2079.56 1810.58,2079.76 1814.57,2079.98 \n",
       "  1818.56,2080.24 1822.54,2080.54 1826.53,2081.14 1830.52,2081.98 1834.51,2082.86 1838.49,2083.49 1842.48,2083.75 1846.47,2083.82 1850.45,2083.93 1854.44,2084.23 \n",
       "  1858.43,2084.69 1862.42,2085.19 1866.4,2085.63 1870.39,2086.06 1874.38,2086.54 1878.37,2086.99 1882.35,2087.43 1886.34,2087.72 1890.33,2087.92 1894.32,2088.1 \n",
       "  1898.3,2088.32 1902.29,2088.72 1906.28,2089.2 1910.27,2089.66 1914.25,2090.07 1918.24,2090.38 1922.23,2090.73 1926.22,2091.08 1930.2,2091.48 1934.19,2091.88 \n",
       "  1938.18,2092.25 1942.17,2092.61 1946.15,2092.94 1950.14,2093.24 1954.13,2093.49 1958.12,2093.77 1962.1,2094.05 1966.09,2094.37 1970.08,2094.7 1974.07,2095.08 \n",
       "  1978.05,2095.44 1982.04,2095.77 1986.03,2096.05 1990.02,2096.24 1994,2096.3 1997.99,2096.21 2001.98,2095.97 2005.97,2095.63 2009.95,2095.36 2013.94,2095.32 \n",
       "  2017.93,2095.9 2021.92,2096.93 2025.9,2098.22 2029.89,2099.23 2033.88,2099.54 2037.87,2099.33 2041.85,2099.09 2045.84,2099.13 2049.83,2099.77 2053.82,2100.74 \n",
       "  2057.8,2101.46 2061.79,2101.69 2065.78,2101.65 2069.77,2101.62 2073.75,2101.79 2077.74,2102.31 2081.73,2102.98 2085.72,2103.49 2089.7,2103.8 2093.69,2103.9 \n",
       "  2097.68,2103.97 2101.67,2104.19 2105.65,2104.56 2109.64,2105 2113.63,2105.42 2117.62,2105.77 2121.6,2106 2125.59,2106.19 2129.58,2106.4 2133.56,2106.64 \n",
       "  2137.55,2106.91 2141.54,2107.23 2145.53,2107.56 2149.51,2107.88 2153.5,2108.16 2157.49,2108.4 2161.48,2108.59 2165.46,2108.78 2169.45,2109.06 2173.44,2109.4 \n",
       "  2177.43,2109.73 2181.41,2110.08 2185.4,2110.35 2189.39,2110.59 2193.38,2110.81 2197.36,2110.98 2201.35,2111.17 2205.34,2111.4 2209.33,2111.63 2213.31,2111.87 \n",
       "  2217.3,2112.16 2221.29,2112.42 2225.28,2112.65 2229.26,2112.88 2233.25,2113.03 2237.24,2113.15 2241.23,2113.42 2245.21,2113.74 2249.2,2114.05 2253.19,2114.43 \n",
       "  2257.18,2114.73 2261.16,2115.04 2265.15,2115.33 2269.14,2115.61 2273.13,2115.82 2277.11,2116.01 2281.1,2116.25 2285.09,2116.49 2289.08,2116.73 2293.06,2116.96 \n",
       "  \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr(size=(600,600))\n",
    "p_l_curve = plot(1:500 , loss_history ,\n",
    "       xlabel = \"Epochs\",\n",
    "       ylabel=\"Loss\",\n",
    "      title=\"Learning Curve \",\n",
    "        legend = false ,\n",
    "    color=:blue ,\n",
    "    linewidth = 2 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c37a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.4",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
